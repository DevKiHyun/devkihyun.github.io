---
title: "머신러닝과 확률"
categories:
  - study
tags:
  - probability
  - statics
  - machine learning
  - deep learning
last_modified_at: 2018-10-19T14:27:00+09:00
comments : true
---

안녕하세요. 이번 시간에는 머신러닝과 확률을 얘기하려고 합니다. 최소한 [모두의 딥러닝](https://hunkim.github.io/ml/) 강의를 이수한 수준은 필요로 하고 기본적으로 고등학교 수준의 확률과 통계를 알고 있다고 여기고 진행하겠습니다.

# 확률론적 관점

최근엔 잘 만들어진 딥러닝 프레임워크들이 많아 코딩만 할 줄 알면 딥러닝에 대해 깊이 있게 몰라도 데이터와 코드 몇 줄만으로도 그럴듯한 결과물을 얻을 수가 있습니다. 그래서 딥러닝 개발의 진입 장벽이 많이 낮아졌다고 할 수 있죠. 그 과정에서 선형대수에 관련된 문서나 글을 읽을 일이 많아 선형대수가 중요하다는 것은 알겠는데, 확률과 통계는 그럼 진짜 중요한가? 하는 의문이 들기도 합니다. 중요하다는 얘기는 많이 들었는데 정작 체감을 못 하기 때문입니다. 그래서 이번엔 우리가 알고 있는 딥러닝 모델을 확률론적인 관점에서 해석해보고자 합니다.

__분류 문제(Classification)__ 를 예시로 한번 들어보겠습니다. 우리가 딥러닝으로는 제일 먼저 해보게 되는 게 아마 __MNIST 숫자 손글씨 데이터__ 로 해당 숫자를 분류해보는 과제일 겁니다. MNIST는 __숫자 데이터 X와 클래스 Y(라벨)가 쌍으로 구성된 데이터셋__ 입니다. 단순하게 베이직한 뉴럴네트워크로 이루어져 있는 모델이 있다고 하면, 우리가 입력값으로 X를 넣으면, 모델이 계산한 결과(Output)를 Y랑 비교하면서 잘 분류가 되도록 학습됩니다. 즉, 이 모델은 데이터 X를 잘 이해해서 해당 클래스 Y를 *'예측'* 할 수 있는 *'함수'* 를 알아내는 겁니다. 보통은 이 학습과정을 선형대수로도 충분히 설명할 수 있습니다. 지금까지 설명한 내용을 아래의 그림으로 간단히 나타낼 수 있습니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/0.PNG" alt="">
  <figcaption>그림출처 http://sanghyukchun.github.io/58/</figcaption>
</figure> 

여기서 알고리즘 A가 바로 데이터를 잘 분석한 *'함수'* 가 되는 것입니다. 근데 이 *'함수'* 부분을 __확률론적 관점__ 으로 해석해 볼 수가 있습니다. 본격적으로 확률론적 관점을 설명하기 앞서 몇 가지 확률 개념을 소개하도록 하겠습니다.

### 확률 변수 (Random Variable)
확률 변수(Random Variable)는 표본공간에서 일정한 확률을 가지고 발생하는 사건에 수치를 일대일 대응시키는 __함수__ 입니다. __함수__ 라는 것이 중요한 부분입니다. 예를 들면 동전 하나를 한번 던지면 앞면 또는 뒷면 총 2가지 경우가 발생가능합니다. 이 2가지의 경우가 사건(event)가 되는거죠. 확률 변수를 X라 하면 이것을 이용해 앞면(H)과 뒷면(T)를 각각 특정한 수치로 일대일 대응을 하게 됩니다. X(H) = 1, X(T) = 0 로 말이죠. 그리고 나서 우리는 그 확률변수 X가 갖는 특정 값 x를 확률 P를 통해 확률 값 __P(x)__ 을 얻을 수 있게 됩니다.  0과 1은 이산적(discrete)이기 때문에 방금과 같은 확률변수 X는 __이산확률변수(discrete random variable)__ 입니다. 만약 확률변수 X가 연속적(continuous)이면 __연속확률변수(continuous random variable)__ 이죠.

그럼 잠시 돌아와서 __mnist 분류 문제__ 를 다시 떠올려 봅시다. 우리의 분류모델을 말로 설명해 보면 *'입력값 X를 넣어주면 모델에서 Y가 나올 확률'* 이라고 할 수 있습니다. 그렇다면 Y를 __사건(event)__ 이라 볼 수 있습니다. 그리고 나서 Y를 어느 특정 수치들로 일대일 매핑을 해주면 그 과정이 확률변수가 되는거죠. 근데 조금 찝찝한 부분이 남아 있습니다. 그럼 입력값 X는 뭐로 이해하면 될까요? 그건 이제 뒤에서 다시 얘기하도록 하고 이 정도까지만 이해하고 넘어가도록 하겠습니다.

### 확률 분포 (Probability Distribution)
우리는 사건(event)을 확률변수 X를 통해 특정 값들로 매핑해준 뒤 그 특정 값 x를 확률 P에 넣어 P(x), 즉 확률 값을 얻는 다는 것을 이해했습니다. 확률 분포는 이 확률변수 X에 대한 확률 값들이 어떻게 분포되어 있는지를 말해줍니다. 그림을 보시면 바로 이해되실겁니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/1.png" alt="">
  <figcaption>그림출처 https://www.statisticsfromatoz.com/blog/statistics-tip-of-the-week-different-distributions-can-have-discrete-or-continuous-probability-graphs-for-discrete-or-continuous-data</figcaption>
</figure> 

확률 P가 있다고 합시다. 우리가 만약 그 확률 P에 *셀 수 있는* __이산확률변수__ 를 넣어주면 그래프엔 그 해당 확률 값들이 __점__ 처럼 찍히게 됩니다. 또는 왼쪽 그림처럼 히스토그램처럼 그릴 수도 있구요. 하지만 *셀 수 없는* __연속확률변수__ 를 함수 P에 넣어주면 점이 모여서 __선__ 이 되는 것처럼 오른쪽 그림의 분포를 그리게 됩니다. 즉, 확률 분포는 확률 변수를 입력으로 받은 확률 P의 모습이 되기 때문에 우리가 흔히 알고 있던 __확률 P가 확률 분포랑 똑같은 것__ 입니다. 요컨데 *P(X) 는 확률변수 X에 대한 확률 분포다* 라고 우리는 말 할 수 있습니다.
