---
title: "머신러닝과 확률"
categories:
  - study
tags:
  - probability
  - statics
  - machine learning
  - deep learning
last_modified_at: 2018-10-19T14:27:00+09:00
comments : true
mathjax: true
---

안녕하세요. 이번 시간에는 머신러닝과 확률을 얘기하려고 합니다. 최소한 김성훈 교수님의 [모두의 딥러닝](https://hunkim.github.io/ml/) 강의를 이수한 수준은 필요로 하고 기본적으로 고등학교 수준의 확률과 통계를 알고 있다고 여기고 진행하겠습니다.

# 확률론적 관점

최근엔 잘 만들어진 딥러닝 프레임워크들이 많아 코딩만 할 줄 알면 딥러닝에 대해 깊이 있게 몰라도 데이터와 코드 몇 줄만으로도 그럴듯한 결과물을 얻을 수가 있습니다. 그래서 딥러닝 개발의 진입 장벽이 많이 낮아졌다고 할 수 있죠. 그 과정에서 선형대수에 관련된 문서나 글을 읽을 일이 많아 선형대수가 중요하다는 것은 알겠는데, 확률과 통계는 그럼 진짜 중요한가? 하는 의문이 들기도 합니다. 중요하다는 얘기는 많이 들었는데 정작 체감을 못 하기 때문입니다. 그래서 이번엔 우리가 알고 있는 딥러닝 모델을 확률론적인 관점에서 해석해보고자 합니다.

__분류 문제(Classification)__ 를 예시로 한번 들어보겠습니다. 우리가 딥러닝으로는 제일 먼저 해보게 되는 게 아마 __MNIST 숫자 손글씨 데이터__ 로 해당 숫자를 분류해보는 과제일 겁니다. MNIST는 __숫자 데이터 X와 클래스 Y(라벨)가 쌍으로 구성된 데이터셋__ 입니다. 단순하게 베이직한 뉴럴네트워크로 이루어져 있는 모델이 있다고 하면, 우리가 입력값으로 X를 넣으면, 모델이 계산한 결과(Output)를 Y랑 비교하면서 잘 분류가 되도록 학습됩니다. 즉, 이 모델은 데이터 X를 잘 이해해서 해당 클래스 Y를 *'예측'* 할 수 있는 *'함수'* 를 알아내는 겁니다. 보통은 이 학습과정을 선형대수로도 충분히 설명할 수 있습니다. 지금까지 설명한 내용을 아래의 그림으로 간단히 정리해보겠습니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/0.PNG" alt="">
  <figcaption>그림출처 http://sanghyukchun.github.io/58/</figcaption>
</figure> 

여기서 알고리즘 A가 바로 데이터를 잘 분석한 *'함수'* 가 되는 것입니다. 근데 이 *'함수'* 부분을 __확률론적 관점__ 으로 해석해 볼 수가 있습니다. 본격적으로 확률론적 관점을 설명하기 앞서 몇 가지 확률 개념을 소개하도록 하겠습니다.

### 확률 변수 (Random Variable)

확률 변수(Random Variable)는 표본공간에서 일정한 확률을 가지고 발생하는 __사건(event)__ 에 수치를 일대일 대응시키는 __함수__ 입니다. __함수__ 라는 것이 중요한 부분입니다. 예를 들면 동전 하나를 한번 던지면 앞면 또는 뒷면 총 2가지 경우가 발생가능합니다. 이 2가지의 경우가 사건가 되는거죠. 확률 변수를 X라 하면 이것을 이용해 앞면(H)과 뒷면(T)를 각각 특정한 수치로 일대일 대응을 하게 됩니다. X(H) = 1, X(T) = 0 로 말이죠. 0과 1은 이산적(discrete)이기 때문에 방금과 같은 확률변수 X는 __이산확률변수(discrete random variable)__ 입니다. 만약 확률변수 X가 연속적(continuous)이면 __연속확률변수(continuous random variable)__ 이죠. 데이터도 마찬가지 입니다. 어떠한 사건이 발생했으니깐 그게 데이터로 남는거겠죠? 따라서 데이터는 사건이라는 것을 명심하기 바랍니다. 

그럼 잠시 돌아와서 __mnist 분류 문제__ 를 다시 떠올려 봅시다. 우리의 분류모델을 말로 설명해 보면 *'입력값 X를 넣어주면 모델에서 Y가 나올 확률'* 이라고 할 수 있습니다. 그렇다면 Y를 __사건(event)__ 이라 볼 수 있습니다. 그리고 나서 Y를 어느 특정 수치들로 일대일 매핑을 해주면 그 과정이 확률변수가 되는거죠. 근데 조금 찝찝한 부분이 남아 있습니다. 그럼 입력값 X는 뭐로 이해하면 될까요? 그건 이제 뒤에서 다시 얘기하도록 하고 이 정도까지만 이해하고 넘어가도록 하겠습니다.

### 확률 (Probability)

확률(Probability)는 해당 사건이 일어날 가능성을 의미합니다. __확률__ 이라는 개념은 관점에 따라 설명하는게 달라집니다. 기존의 우리가 잘 알고 있는 확률의 개념은 해당 사건의 빈도수를 따져 그 사건이 발생할 가능성을 의미합니다. 보통 실험적인 성향을 갖는 반복되는 시도를 통해 사건의 빈도수를 측정해서(ex. 동전 던지기, 주사위 던지기) __확률 P__ 를 계산합니다. 이런 경우를 __Frequentist probability__ 라고 합니다. 다른 관점으로는 __Bayesian Probability__ 가 있습니다. 확률을 *지식 또는 믿음의 정도를 나타내는 양* 으로 해석하는 확률론입니다. 후자의 확률론은 뒤부분에서 설명하겠습니다. 

확률 변수 X는 사건(event)를 특정 실수 값인 x로 매핑을 해줍니다. 그리고 나서 그 x를 확률 P에 넣어주면 바로 해당 사건에 대한 가능성을 나타내는 확률값이 나오게 됩니다. 따라서 확률 P는 확률변수를 입력으로 받는 또 하나의 __함수__ 인 것입니다.

### 확률 분포 (Probability Distribution)

우리는 사건(event)을 확률변수 X를 통해 특정 값들로 매핑해준 뒤 그 특정 값 x를 확률 P에 넣어 P(x), 즉 확률 값을 얻는 다는 것을 이해했습니다. 확률 분포는 이 확률변수 X에 대한 확률 값들이 어떻게 분포되어 있는지를 말해줍니다. 그림을 보시면 바로 이해되실겁니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/1.png" alt="">
  <figcaption>그림출처 https://www.statisticsfromatoz.com/blog/statistics-tip-of-the-week-different-distributions-can-have-discrete-or-continuous-probability-graphs-for-discrete-or-continuous-data</figcaption>
</figure> 

확률 P가 있다고 합시다. 우리가 만약 그 확률 P에 *셀 수 있는* __이산확률변수__ 를 넣어주면 그래프엔 그 해당 확률 값들이 __점__ 처럼 찍히게 됩니다. 또는 왼쪽 그림처럼 히스토그램처럼 그릴 수도 있구요. 하지만 *셀 수 없는* __연속확률변수__ 를  P에 넣어주면 점이 모여서 __선__ 이 되는 것처럼 오른쪽 그림의 분포를 그리게 됩니다. 즉, 확률 분포는 확률 변수를 입력으로 받은 확률 P의 모습이 되기 때문에 우리가 흔히 알고 있던 __확률 P가 확률 분포랑 똑같은 것__ 입니다. 요컨데 *P(X) 는 확률변수 X에 대한 확률 분포다* 라고 우리는 말 할 수 있습니다. 

이산확률분포는 확률 질량 함수, Probability mass function(PMF)로 나타냅니다. 이 확률 함수가 우리가 알고 있는 P인 것이죠. 연속확률분포는 확률 밀도 함수, Probability density function(PDF)로 나타냅니다. 이산확률분포는 특정 확률변수 x 에 대한 정확한 확률값을 표현할 수 있습니다. 반면에 연속확률분포는 셀 수 없는 확률 변수들의 분포이기 때문에 특정 확률 변수 x에 대한 정확한 확률값을 표현할 수 없습니다. 그 대신 정해진 구간 a<x<b 에서의 수치를 표현합니다.

우리가 사건을 예측하기 위해서 해당 사건의 확률을 계산합니다. 그러기 위해선 결국 확률 분포를 구해야 됩니다. 머신러닝의 학습과정이 바로 그 확률 분포를 찾는 것입니다. 일단 여기서 한번 그림으로 우리 지금까지 짚고 넘어온 개념들을 정리해보겠습니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/2.png" alt="">
</figure> 

### 조건부 확률 (Conditional Probability)

조건부 확률의 사전적 정의는 어떤 사건이 일어난 조건 하에서 다른 사건이 일어날 확률이라고 정의합니다. 하지만 이렇게 말하기 보다는 *'어떤 사건이 일어날 확률을 구하는 과정에서 또 다른 사건을 단서로 제공한다'* 고 말하는게 더 옳습니다. 제공되는(given) 사건이 A라고 하고 우리가 구하고자 하는 사건 B에 대한 조건부 확률을 기호로 나타내면 P(BㅣA) 라고 할 수 있습니다. 우리의 관심사는 B이고 그 단서로 A를 고려를 하는 거죠.

잠시 머신러닝 얘기로 돌아오도록 하죠. 우리는 앞에서 분류모델을 *'입력값 X를 넣어주면 모델에서 Y가 나올 확률'*  이라고 표현했습니다. 근데 Y가 나올 확률이기 때문에 Y는 확률 변수라고 정했는데, 입력값 X은 뭐라고 해야 될지 애매했습니다. 벌써 눈치채신 분들도 있겠지만 이제 우리는 조건부 확률을 가지고서 분류모델을 부족함 없이 표현할 수 있게 되었습니다. 바로 단서가 입력값 X가 되는 것입니다. 이제 드디어 *'입력값 X를 넣어주면 모델에서 Y가 나올 확률'* 을 아래처럼 표현 할 수 있습니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/3.png" alt="">
</figure> 

### 가능도 (Likelihood)

우리가 만약 확률 분포를 알고 있다면, 우리는 해당 분포에 확률 변수를 넣어 바로 확률을 계산할 수 있습니다. 하지만 현실에선 데이터가 굉장히 많고 또 데이터 차원이 높은 경우가 많아 확률 분포를 알아내는 것이 쉽지가 않습니다. 실전에선 관측 값(데이터)만 주어지고 확률 분포를 알아내야 되는 경우가 거의 대부분이라고 생각하시면 됩니다. 이 얘기는 이제 우리는 확률 분포가 뭔지 모르니깐 어떠한 특정 확률 분포를 미리 __가정__ 을 하고 시작하자는 것입니다. 그리고 *'그 가정한 확률 분포에다가 우리한테 주어진 관측값을 넣었을 때 나오는 확률값'* 을 __Likelihood__ 라고 합니다.

그럼 __Likelihood__ 를 어떻게 이용해야 될까요? 어떠한 확률 분포가 있다고 하면 분명 그 확률 분포를 구성하는 __Parameter__ 가 있을 것입니다.
예를 들어 y 라는 확률 분포가 y = ax+b 라는 방정식을 이루고 있다고 가정해봅시다. 그러면 x는 우리한테 주어진 관측값이라고 할 때 y의 값은 __Likelihood__ 가 되고 그 값이 어떻게 나올지는 a 와 b가 결정하게 됩니다. 그럼 a와 b가 y 라는 확률 분포의 __Parameter__ 가 되는 것입니다. __Parameter__ 는 언제든 바뀔 수 있는 *'변수'* 입니다. 이 __Parameter__ 가 어떤 값을 갖는지에 따라 해당 확률 분포의 모양이 결정되는 것이죠. __Likelihood__ 의 목적은 사실 __Parameter__ 에 있습니다. 우리가 확률 분포를 가정한 이유는 실제 데이터에 대한 확률 분포를 알 지 못하기 때문입니다. 그럼 확률 분포를 가정하고 나서 __Parameter__ 를 관측한 데이터를 잘 표현하도록 조절하면 실제 확률 분포와 비슷하지 않을까요? 그럼 잘 표현하려면 어떻게 해야될까요? 우리는 확률 분포를 추정하기 위한 방법 중에서 __최대우도추정법(MLE, Maximum Likelihood Estimation)__ 사용하도록 하겠습니다. 

### 최대우도추정법 (Maximum Likelihood Estimation)

__MLE__ 는 각 관측값에 대한 총 가능도(모든 가능도의 곱)가 최대가 되는 분포를 찾는 추정법입니다. 원리는 굉장히 간단합니다. 가정한 분포를 이루는 __Parameter__ 를 업데이트 해가면서 분포를 움직인다고 생각하시면 됩니다. 계속해서 움직이다 보면 분명 __Likelihood__ 가 제일 큰 부분에 오게 될테고 우리는 잘 추정했다고 결론 내리는 것입니다.

딥러닝도 역시 가설을 세우고 그 가설이 최대화 되는 __Parameter__ 를 찾도록 역전파(Backpropagation) 방식으로 통해 __Parameter__ 를 업데이트 합니다. 따라서 확률론적인 관점으로 보면 __MLE__ 를 이용해서 확률 모델을 최대화 하는 게 가능한 것입니다.

이제 우리는 *'실제 확률 분포는 모르지만 분명 뭔가 있을거다*' 라고 __확신__ 을 갖고 임의의 확률 분포를 가정하기로 약속했습니다. 그러면 어떻게 가정을 해야되는 걸 까요? 다행히 우리는 그냥 유명한 확률 분포들을 가져다가 가정해도 충분합니다. 이산 확률 분포로는 푸아송 분포, 이항 분포, 베르누이 분포 등이 있고 연속확률분포는 유명한 __가우시안 분포(Gaussian Distribution)__ 이 있습니다. 가우시안 분포는 정규 분포(Normal Distribution) 라고도 합니다.  

### 정규 분포 (Normal Distribution = Gaussian Distribution)

통계학에서 정규 분포는 굉장히 자주 쓰이는 중요한 분포입니다. 자연계에서 발생하는 현상들 중 거의 대부분은 분포가 정규 분포로 매우 가깝게 표현되는 경우가 많기 때문이죠. 그래서 확률 분포를 가정해야 되는 경우에는 대부분 정규 분포로 가정해도 무방합니다. 우리가 딥러닝을 공부하면서 다룰 데이터들 또한 자연계에서 관측된 데이터들인 경우가 많습니다. 따라서 이에 대한 분포를 모를 경우엔 과감하게 정규 분포로 가정해도 크게 문제가 되진 않습니다. 그리고 정규 분포의 또 하나의 장점은 평균(μ) 과 표준편차(σ)만 알면 언제든지 정규 분포를 그릴 수 있다는 점입니다. 즉, 정규 분포를 구성하는 __Parameter__ 가 평균(μ) 과 표준편차(σ)가 되는 것이죠. 그럼 바로 전에 설명했던 __MLE__ 에서 우리가 가정한 분포가 정규 분포라고 하면 __Likelihood__ 가 최대화 되도록 업데이트 하는 __Parameter__ 는 당연히 평균과 표준편차가 되는 것입니다. 이제 그림으로 한번 정리해 보겠습니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/4.png" alt="">
</figure>

그림에서 Y는 확률 변수를 의미합니다. 그리고 그 확률변수 Y에 대해 실제 확률 분포(모르지만)는
$$ f_{\R}(Y) $$
이고 
