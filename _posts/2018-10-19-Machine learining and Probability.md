---
title: "머신러닝과 확률"
categories:
  - study
tags:
  - probability
  - statics
  - machine learning
  - deep learning
last_modified_at: 2018-10-19T14:27:00+09:00
comments : true
---

안녕하세요. 이번 시간에는 머신러닝과 확률을 얘기하려고 합니다. 최소한 김성훈 교수님의 [모두의 딥러닝](https://hunkim.github.io/ml/) 강의를 이수한 수준은 필요로 하고 기본적으로 고등학교 수준의 확률과 통계를 알고 있다고 여기고 진행하겠습니다.

# 확률론적 관점

최근엔 잘 만들어진 딥러닝 프레임워크들이 많아 코딩만 할 줄 알면 딥러닝에 대해 깊이 있게 몰라도 데이터와 코드 몇 줄만으로도 그럴듯한 결과물을 얻을 수가 있습니다. 그래서 딥러닝 개발의 진입 장벽이 많이 낮아졌다고 할 수 있죠. 그 과정에서 선형대수에 관련된 문서나 글을 읽을 일이 많아 선형대수가 중요하다는 것은 알겠는데, 확률과 통계는 그럼 진짜 중요한가? 하는 의문이 들기도 합니다. 중요하다는 얘기는 많이 들었는데 정작 체감을 못 하기 때문입니다. 그래서 이번엔 우리가 알고 있는 딥러닝 모델을 확률론적인 관점에서 해석해보고자 합니다.

__분류 문제(Classification)__ 를 예시로 한번 들어보겠습니다. 우리가 딥러닝으로는 제일 먼저 해보게 되는 게 아마 __MNIST 숫자 손글씨 데이터__ 로 해당 숫자를 분류해보는 과제일 겁니다. MNIST는 __숫자 데이터 X와 클래스 Y(라벨)가 쌍으로 구성된 데이터셋__ 입니다. 단순하게 베이직한 뉴럴네트워크로 이루어져 있는 모델이 있다고 하면, 우리가 입력값으로 X를 넣으면, 모델이 계산한 결과(Output)를 Y랑 비교하면서 잘 분류가 되도록 학습됩니다. 즉, 이 모델은 데이터 X를 잘 이해해서 해당 클래스 Y를 *'예측'* 할 수 있는 *'함수'* 를 알아내는 겁니다. 보통은 이 학습과정을 선형대수로도 충분히 설명할 수 있습니다. 지금까지 설명한 내용을 아래의 그림으로 간단히 나타낼 수 있습니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/0.PNG" alt="">
  <figcaption>그림출처 http://sanghyukchun.github.io/58/</figcaption>
</figure> 

여기서 알고리즘 A가 바로 데이터를 잘 분석한 *'함수'* 가 되는 것입니다. 근데 이 *'함수'* 부분을 __확률론적 관점__ 으로 해석해 볼 수가 있습니다. 본격적으로 확률론적 관점을 설명하기 앞서 몇 가지 확률 개념을 소개하도록 하겠습니다.

### 확률 변수 (Random Variable)
확률 변수(Random Variable)는 표본공간에서 일정한 확률을 가지고 발생하는 사건에 수치를 일대일 대응시키는 __함수__ 입니다. __함수__ 라는 것이 중요한 부분입니다. 예를 들면 동전 하나를 한번 던지면 앞면 또는 뒷면 총 2가지 경우가 발생가능합니다. 이 2가지의 경우가 사건(event)가 되는거죠. 확률 변수를 X라 하면 이것을 이용해 앞면(H)과 뒷면(T)를 각각 특정한 수치로 일대일 대응을 하게 됩니다. X(H) = 1, X(T) = 0 로 말이죠. 0과 1은 이산적(discrete)이기 때문에 방금과 같은 확률변수 X는 __이산확률변수(discrete random variable)__ 입니다. 만약 확률변수 X가 연속적(continuous)이면 __연속확률변수(continuous random variable)__ 이죠.

그럼 잠시 돌아와서 __mnist 분류 문제__ 를 다시 떠올려 봅시다. 우리의 분류모델을 말로 설명해 보면 *'입력값 X를 넣어주면 모델에서 Y가 나올 확률'* 이라고 할 수 있습니다. 그렇다면 Y를 __사건(event)__ 이라 볼 수 있습니다. 그리고 나서 Y를 어느 특정 수치들로 일대일 매핑을 해주면 그 과정이 확률변수가 되는거죠. 근데 조금 찝찝한 부분이 남아 있습니다. 그럼 입력값 X는 뭐로 이해하면 될까요? 그건 이제 뒤에서 다시 얘기하도록 하고 이 정도까지만 이해하고 넘어가도록 하겠습니다.

### 확률 (Probability)
확률(Probability)는 해당 사건이 일어날 가능성을 의미합니다. __확률__ 이라는 개념은 관점에 따라 설명하는게 달라집니다. 기존의 우리가 잘 알고 있는 확률의 개념은 해당 사건의 빈도수를 따져 그 사건이 발생할 가능성을 의미합니다. 보통 실험적인 성향을 갖는 반복되는 시도를 통해 사건의 빈도수를 측정해서(ex. 동전 던지기, 주사위 던지기) __확률 P__ 를 계산합니다. 이런 경우를 __Frequentist probability__ 라고 합니다. 다른 관점으로는 __Bayesian Probability__ 가 있습니다. 확률을 *지식 또는 믿음의 정도를 나타내는 양* 으로 해석하는 확률론입니다. 후자의 확률론은 뒤부분에서 설명하겠습니다. 

확률 변수 X는 사건(event)를 특정 실수 값인 x로 매핑을 해줍니다. 그리고 나서 그 x를 확률 P에 넣어주면 바로 해당 사건에 대한 가능성을 나타내는 확률값이 나오게 됩니다. 따라서 확률 P는 확률변수를 입력으로 받는 또 하나의 __함수__ 인 것입니다.

### 확률 분포 (Probability Distribution)
우리는 사건(event)을 확률변수 X를 통해 특정 값들로 매핑해준 뒤 그 특정 값 x를 확률 P에 넣어 P(x), 즉 확률 값을 얻는 다는 것을 이해했습니다. 확률 분포는 이 확률변수 X에 대한 확률 값들이 어떻게 분포되어 있는지를 말해줍니다. 그림을 보시면 바로 이해되실겁니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/1.png" alt="">
  <figcaption>그림출처 https://www.statisticsfromatoz.com/blog/statistics-tip-of-the-week-different-distributions-can-have-discrete-or-continuous-probability-graphs-for-discrete-or-continuous-data</figcaption>
</figure> 

확률 P가 있다고 합시다. 우리가 만약 그 확률 P에 *셀 수 있는* __이산확률변수__ 를 넣어주면 그래프엔 그 해당 확률 값들이 __점__ 처럼 찍히게 됩니다. 또는 왼쪽 그림처럼 히스토그램처럼 그릴 수도 있구요. 하지만 *셀 수 없는* __연속확률변수__ 를  P에 넣어주면 점이 모여서 __선__ 이 되는 것처럼 오른쪽 그림의 분포를 그리게 됩니다. 즉, 확률 분포는 확률 변수를 입력으로 받은 확률 P의 모습이 되기 때문에 우리가 흔히 알고 있던 __확률 P가 확률 분포랑 똑같은 것__ 입니다. 요컨데 *P(X) 는 확률변수 X에 대한 확률 분포다* 라고 우리는 말 할 수 있습니다. 

이산확률분포는 확률 질량 함수, Probability mass function(PMF)로 나타냅니다. 이 확률 함수가 우리가 알고 있는 P인 것이죠. 연속확률분포는 확률 밀도 함수, Probability density function(PDF)로 나타냅니다. 이산확률분포는 특정 확률변수 x 에 대한 정확한 확률값을 표현할 수 있습니다. 반면에 연속확률분포는 셀 수 없는 확률 변수들의 분포이기 때문에 특정 확률 변수 x에 대한 정확한 확률값을 표현할 수 없습니다. 그 대신 정해진 구간 a<x<b 에서의 수치를 표현합니다.

우리가 사건을 예측하기 위해서 해당 사건의 확률을 계산합니다. 그러기 위해선 결국 확률 분포를 구해야 됩니다. 머신러닝의 학습과정이 바로 그 확률 분포를 찾는 것입니다. 일단 여기서 한번 그림으로 우리 지금까지 짚고 넘어온 개념들을 정리해보겠습니다.

<figure class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/post_images/2018-10-19-Machine_learning_and_Probability/2.png" alt="">
</figure> 

### 조건부 확률 (Conditional Probability)
조건부 확률의 사전적 정의는 어떤 사건이 일어난 조건 하에서 다른 사건이 일어날 확률이라고 정의합니다. 하지만 이렇게 말하기 보다는 *'어떤 사건이 일어날 확률을 구하는 과정에서 또 다른 사건을 단서로 제공한다'* 고 말하는게 더 옳습니다. 제공되는(given) 사건이 A라고 하고 우리가 구하고자 하는 사건 B에 대한 조건부 확률을 기호로 나타내면 P(B|A) 라고 할 수 있습니다. 우리의 관심사는 B이고 그 단서로 A를 고려를 하는 거죠.

잠시 머신러닝 얘기로 돌아오도록 하죠. 우리는 앞에서 분류모델을 *'입력값 X를 넣어주면 모델에서 Y가 나올 확률'*  이라고 표현했습니다. 근데 Y가 나올 확률이기 때문에 Y는 확률 변수라고 정했는데, 입력값 X은 뭐라고 해야 될지 애매했습니다. 벌써 눈치채신 분들도 있겠지만 이제 우리는 조건부 확률을 가지고서 분류모델을 부족함 없이 표현할 수 있게 되었습니다. 바로 단서가 입력값 X가 되는 것입니다. 이제 드디어 *'입력값 X를 넣어주면 모델에서 Y가 나올 확률'* 을 아래처럼 표현 할 수 있습니다.
